<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Chen Bao's Homepage</title>
  <link href="css/bootstrap.css" rel='stylesheet' type='text/css' />
  <!--<link rel="shortcut icon" href="../images/fav_icon.png" type="image/x-icon">-->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <!-- Bulma Version 0.7.5-->
  <link rel="stylesheet" href="https://unpkg.com/bulma@0.7.5/css/bulma.min.css" />
  <link href='css/style.css' rel='stylesheet' type='text/css'>
</head>

<body>
  <section class="section">
    <div class="container">
      <div class="columns">
        <div class="column is-2">
          <div class="sticky">
            <figure class="image is-128x128">
              <img class="is-rounded" src="/images/sfphoto.jpg" style="object-fit: cover;">
            </figure>
            <div class="content">

              <h3></h3>
              <h3>Brynn(Yibo) Peng</h3>
              <h3>ÂΩ≠Ëâ∫Âçö</h3>
            </div>
            <!-- details -->
            <div class="details">
              <h3>EMAIL</h3>
              <p><a href="mailto:yibop@andrew.cmu.edu">yibop<br>[at]andrew[dot]cmu[dot]edu</a></p>
            </div>
            <!-- social network icons -->
            <div class="social">
              <a href="https://github.com/pppyb" target="_blank">
                <span class="fab fa-github fa-2x" style="display:inline; text-decoration: none"></span>
              </a>
              <!-- <a href="https://scholar.google.com/citations?user=HOngPZAAAAAJ" target="_blank">
                <span class="fab fa-google fa-2x" style="display:inline; text-decoration: none"></span>
              </a> -->
              <a href="https://www.linkedin.com/in/yibo-peng-cs/" target="_blank">
                <span class="fab fa-linkedin fa-2x" style="display:inline; text-decoration: none"></span>
              </a>
              <!--<a href="https://www.facebook.com/???????" target="_blank">-->
                <!--<span class="fab fa-facebook fa-2x" style="display:inline; text-decoration: none"></span>-->
              <!--</a>-->
              <a href="https://x.com/BrynnPeng" target="_blank">
                <i class="fab fa-twitter fa-2x" style="display:inline; text-decoration: none"></i>
              </a>
            </div>


            <div id="sidebar" class="menu sticky is-hidden-mobile">
              <p class="menu-label"><b>Quick Links</b></p>
              <ul class="menu-list">
                <li><a href="#news">News</a></li>
                <li><a href="#intro">Intro</a></li>
                <li><a href="#research">Research</a></li>
                <!-- <li><a href="blog">Blog</a></li> -->
              </ul>
            </div>
          </div>



        </div>
        <div class="column">
          <div class="content">
            <h3 id="news">
              News
            </h3>
            <ul>
              <!-- <li><span>[2024/05] üéâ <a href="https://github.com/haosulab/ManiSkill">ManiSkill3</a> Beta has been released! State-of-the-art fully open-source robotics simulation with the fastest parallel visual simulation, 10-1000x faster than other simulators.</span></li>
              <li><span>[2023/11] üéâ One paper wins the outstanding paper award at the Language Robotics workshop CoRL 2023 and gets accepted to ICLR 2024 as Spotlight. </span></li> -->
              <li><span>[2023/08] üéâ I joined <a href="https://www.ece.cmu.edu/">CMU Electrical and Computer Engineering Department</a> as an Artificial Intelligence Engineering graduate student.</span></li>
              <!-- <li><span>[2023/07] üéâ I win the 2nd place in all three tracks of <a href="https://sapien.ucsd.edu/challenges/maniskill/">ManiSkill2 Challenge</a>. </span></li>
              <li><span>[2023/05] üéâ One paper gets accepted to CVPR 2023.</span></li>
              <li><span>[2022/09] üéâ One paper gets accepted to CoRL 2022 (Oral).</span></li> -->
            </ul>

            <h3 id="intro">Intro</h3>
            <p>I am a second-year master student in the Robotics Institute at Carnegie Mellon University, working with an amazing PhD <a href="https://zorazrw.github.io/"target="_blank" style="text-decoration: underline;">Zora (Zhiruo) Wang</a> on Retrieval-Augmented Generation(RAG), long context model and code genenration.
              <!-- I am also working as an intern supervised by Prof. <a href="https://xiaolonw.github.io/index.html" target="_blank" style="text-decoration: underline;">Xiaolong Wang</a> at the University of California San Diego. -->
              Previously, I got double Bachelor‚Äôs at Beijing Jiaotong University and Lancaster University with major in Computer Science, where I worked with Prof. <a href="https://scholar.google.com/citations?user=RMlrOvQAAAAJ&hl=zh-CN" target="_blank" style="text-decoration: underline;">Jidong Yuan</a> on Data Augmentation.</p>
            <p>
              I am broadly interested in NLP, especially Retrieval-Augmented Generation(RAG), long context model and code genenration. I am currently looking for research assitant with starting time in spring 2025. Feel free to reach out for chat and/or check out my <embed src="files/ra_YIBOP.pdf" type="application/pdf" width="100%" height="600px" />.<br/><br/>
              here.
            </p>
            <h3 id="research">Research</h3>

            <h4>2022</h4>
            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <br>
                  <img src="images/research/2022_GAN.jpg">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <i> Time Series Augmentation Based on GAN</i>
                    <br>
                    ‚Ä¢ Combine unsupervised and supervised learning<br>
                    ‚Ä¢ Capture dynamic and static features of time series<br>
                    ‚Ä¢ Use denoising autoencoder to compose generator<br>
                    ‚Ä¢ Use gated recurrent unit imputation (GRUI) neural
                    <br>
                    <a href="https://www.youtube.com/watch?v=5PyejDxeI24&t=22s" target="_blank">[<u>Demo</u>]</a>
                  </p>
                </div>
              </div>
            </article>
            
                  <!-- </p>
                </div>
              </div>
            </article>

            <h4>2023</h4>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/research/2023_dexart.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <i>DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects</i><br>
                    <b>Chen Bao*</b>, Helin Xu*, Yuzhe Qin, Xiaolong Wang.<br>
                    Computer Vision and Pattern Recognition Conference (CVPR), 2023<br>
                    <a href="https://www.chenbao.tech/dexart/static/paper/dexart.pdf" target="_blank">[<u>Paper</u>]</a>
                    <a href="https://github.com/Kami-code/dexart-release" target="_blank">[<u>Code</u>]</a>
                    <a href="https://www.chenbao.tech/dexart/" target="_blank">[<u>Website</u>]</a>
                    <a href="https://www.youtube.com/watch?v=V_EYQJO1W_U" target="_blank">[<u>Demo</u>]</a>
                  </p>
                </div>
              </div>
            </article>

                       <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/research/2023_gensim.gif">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <i>GenSim: Generating Robotic Simulation Tasks via Large Language Models</i><br>
                    Lirui Wang, Yiyang Ling*, Zhecheng Yuan*, Mohit Shridhar, <b>Chen Bao</b>, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang. <br>
                    Workshop on Language Grounding and Robot Learning (<span style="color: #8C1515">Best Paper</span>), CoRL 2023<br>
                    International Conference on Learning Representations (<span style="color: #8C1515">Spotlight</span>), ICLR 2024<br>


                    <a href="https://arxiv.org/abs/2310.01361" target="_blank">[<u>arXiv</u>]</a>
                    <a href="https://liruiw.github.io/gensim/" target="_blank">[<u>Website</u>]</a>
                    <a href="https://huggingface.co/spaces/Gen-Sim/Gen-Sim" target="_blank">[<u>Demo</u>]</a>
                    <a href="https://github.com/liruiw/GenSim" target="_blank">[<u>Code</u>]</a>
                  </p>
                </div>
              </div>
            </article>
            <h4>2022</h4>

            <article class="columns">
              <div class="column is-3">
                <figure class="image">
                  <img src="images/research/2022_robotube.png">
                </figure>
              </div>
              <div class="column">
                <div class="content">
                  <p>
                    <i>RoboTube: Learning Household Manipulation from
                      Human Videos with Simulated Twin Environments</i><br>
                    Haoyu Xiong*, Haoyuan Fu*, Jieyi Zhang, <b>Chen Bao</b>, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, Cewu Lu.<br>
                    Conference on Robot Learning (<span style="color: #8C1515">Oral</span>), CoRL 2022<br>
                    <a href="https://openreview.net/forum?id=VD0nXUG5Qk" target="_blank">[<u>Paper</u>]</a>
                    <a href="https://www.robotube.org/" target="_blank">[<u>Website</u>]</a>
                  </p>
                </div>
              </div>
            </article> -->

            <!--Research-->
            <!-- <h3 id="Professional Activities">Professional Activities</h3>
            <ul>
              <li>Conference Reviewer: RSS2023</li>
            </ul>

            <h3 id="Selected Awards and Honors">Selected Awards and Honors</h3>
            <ul>
              <li>2023: Shanghai Outstanding Graduate (5%)</li>
              <li>2023: The Second Prize in ManiSkill2 Challenge (6000USD$)</li>
              <li>2022: Yingjiao Scholarship (12000RMB¬•)</li>
              <li>2021: Shaoqiu Scholarship (10000RMB¬•)</li>
              <li>2021: Merit Student of Shanghai Jiao Tong University</li>
              <li>2020: The First Prize of RoboMaster 2020 University Championship</li>
              <li>2020: Junyuan Scholarship (3000RMB¬•)</li>
              <li>2018: The Second Prize of National Olympiad in Informatics in Provinces, Shanghai, China</li>
            </ul> -->

          </div>
        </div>
      </div>
      <p>Credit: web source from <a href="http://hansf.me/">Dr. Songfang Han</a></p>
    </div>
  </section>
  <script>
    var elm = document.querySelector('#sidebar');
    var ms = new MenuSpy(elm, {
      activeClass: 'is-active'
    });
  </script>
  <script src="https://kit.fontawesome.com/970bf0f05d.js" crossorigin="anonymous"></script>
</body>

</html>
